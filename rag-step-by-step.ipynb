{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import FastEmbedEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores.utils import filter_complex_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from langchain_community.vectorstores import Chroma`\n",
    "\n",
    "- Purpose: Imports a vector store called Chroma, responsible for efficiently storing and retrieving text-based data in a way that language models can understand.\n",
    "\n",
    "- Role: Used to create a vectorstore for storing and indexing documents, allowing for efficient retrieval and search capabilities.\n",
    "\n",
    "`from langchain_community.chat_models import ChatOllama`\n",
    "\n",
    "- Purpose: Imports a chat model implementation specifically designed for Ollama, a tool for running large language models locally.\n",
    "\n",
    "- Role: Likely used to enable conversational interactions with a language model through the web app.\n",
    "\n",
    "`from langchain_community.embeddings import FastEmbedEmbeddings`\n",
    "\n",
    "- Purpose: Imports a module for generating text embeddings, which are numerical representations of text that capture its meaning and semantic relationships.\n",
    "\n",
    "- Role: Used to create embeddings for documents and text inputs, allowing for similarity comparisons and other language-related tasks.\n",
    "\n",
    "`from langchain.schema.output_parser import StrOutputParser`\n",
    "\n",
    "- Purpose: Imports a tool for parsing the output of language models, extracting relevant information from their responses.\n",
    "\n",
    "- Role: Used to process and interpret the responses generated by the language model, making them usable within the application.\n",
    "\n",
    "`from langchain_community.document_loaders.csv_loader import CSVLoader`\n",
    "\n",
    "- Purpose: Imports a module specifically designed to load data from CSV files, a common format for storing structured data.\n",
    "\n",
    "- Role: Used to load the CSV document containing the information that will be used for answering questions.\n",
    "\n",
    "`from langchain.text_splitter import RecursiveCharacterTextSplitter`\n",
    "\n",
    "- Purpose: Imports a text splitting tool that can divide text into smaller chunks based on specific criteria.\n",
    "\n",
    "- Role: Might be used to split text into sentences or phrases for further processing or analysis.\n",
    "\n",
    "`from langchain.schema.runnable import RunnablePassthrough`\n",
    "\n",
    "- Purpose: Imports a component likely used for chaining together multiple actions or processes within LangChain.\n",
    "\n",
    "- Role: Might be used to create a sequence of operations involving different LangChain components.\n",
    "\n",
    "`from langchain.prompts import PromptTemplate`\n",
    "\n",
    "- Purpose: Imports a tool for creating prompts, which are instructions or questions given to language models to guide their responses.\n",
    "\n",
    "- Role: Used to formulate the prompts that will be sent to the language model for answering questions about the CSV data.\n",
    "\n",
    "`from langchain.vectorstores.utils import filter_complex_metadata`\n",
    "\n",
    "- Purpose: Imports a utility function for filtering metadata associated with vector store documents.\n",
    "\n",
    "- Role: Might be used to manage or simplify the metadata associated with the stored documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatCSV:\n",
    "    vector_store = None\n",
    "    retriever = None\n",
    "    chain = None\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the question-answering system with default configurations.\n",
    "\n",
    "        This constructor sets up the following components:\n",
    "        - A ChatOllama model for generating responses ('neural-chat').\n",
    "        - A RecursiveCharacterTextSplitter for splitting text into chunks.\n",
    "        - A PromptTemplate for constructing prompts with placeholders for question and context.\n",
    "        \"\"\"\n",
    "        # Initialize the ChatOllama model with 'neural-chat'.\n",
    "        self.model = ChatOllama(model=\"neural-chat\")\n",
    "\n",
    "        # Initialize the RecursiveCharacterTextSplitter with specific chunk settings.\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "\n",
    "        # Initialize the PromptTemplate with a predefined template for constructing prompts.\n",
    "        self.prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            <s> [INST] You are a helpful HR assistant that analyses resumes from different candidates.\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "            Give names when possible. If you don't know the answer,\n",
    "            just say that you don't know.  [/INST] </s> \n",
    "            [INST] Question: {question} \n",
    "            Context: {context} \n",
    "            Answer: [/INST]\n",
    "            \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the class, weâ€™re initializing the attributes for the class.\n",
    "\n",
    "- `self.model` creates an instance `ChatOllama` for processing text\n",
    "\n",
    "- `self.text_splitter` creates an instance of RecursiveCharacterTextSplitter for splitting the text into chunks\n",
    "\n",
    "- `self.prompt` creates an instance of PromptTemplate using a template string\n",
    "\n",
    "Notice the placeholders `{question}` and `{context}`. These will be filled with actual values during runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code defines an ingest method for the same class. The purpose of this method is to ingest data from a CSV file containing resumes, process the data, and set up components for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(self, csv_file_path: str):\n",
    "        '''\n",
    "        Ingests data from a CSV file containing resumes, process the data, and set up the\n",
    "        components for further analysis.\n",
    "\n",
    "        Parameters:\n",
    "        - csv_file_path (str): The file path to the CSV file.\n",
    "\n",
    "        Usage:\n",
    "        obj.ingest(\"/path/to/data.csv\")\n",
    "\n",
    "        This function uses a CSVLoader to load the data from the specified CSV file.\n",
    "\n",
    "        Args:\n",
    "        - file.path (str): The path to the CSV file.\n",
    "        - encoding (str): The character encoding of the file (default is 'utf-8').\n",
    "        - source_column (str): The column in the CSV containing the data (default is \"Resume\").\n",
    "        '''        \n",
    "        loader = CSVLoader(\n",
    "            file_path=csv_file_path,\n",
    "            encoding='utf-8',\n",
    "            source_column=\"Resume\"\n",
    "            )\n",
    "        \n",
    "        # loads the data\n",
    "        data = loader.load()\n",
    "\n",
    "        # splits the documents into chunks\n",
    "        chunks = self.text_splitter.split_documents(data)\n",
    "        chunks = filter_complex_metadata(chunks)\n",
    "\n",
    "        # creates a vector store using embedding\n",
    "        vector_store = Chroma.from_documents(documents=chunks, embedding=FastEmbedEmbeddings())\n",
    "        # sets up the retriever\n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\n",
    "                \"k\": 3,\n",
    "                \"score_threshold\": 0.5,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Define a processing chain for handling a question-answer scenario.\n",
    "        # The chain consists of the following components:\n",
    "        # 1. \"context\" from the retriever\n",
    "        # 2. A passthrough for the \"question\"\n",
    "        # 3. Processing with the \"prompt\"\n",
    "        # 4. Interaction with the \"model\"\n",
    "        # 5. Parsing the output using the \"StrOutputParser\"\n",
    "        self.chain = ({\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "                      | self.prompt\n",
    "                      | self.model\n",
    "                      | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a method for asking questions through a processing chain, ensuring that the chain is set up before attempting to process the question. If the chain is not set up, it returns a message indicating the need to add a CSV document first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def ask(self, query: str):\n",
    "        \"\"\"\n",
    "        Asks a question using the configured processing chain.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The question to be asked.\n",
    "\n",
    "        Returns:\n",
    "        - str: The result of processing the question through the configured chain.\n",
    "        If the processing chain is not set up (empty), a message is returned\n",
    "        prompting to add a CSV document first.\n",
    "        \"\"\"\n",
    "        if not self.chain:\n",
    "            return \"Please, add a CSV document first.\"\n",
    "\n",
    "        return self.chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a method that clears or resets components within a question-answering system, making them `None` and effectively clearing the existing configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Clears the components in the question-answering system.\n",
    "\n",
    "        This method resets the vector store, retriever, and processing chain to None,\n",
    "        effectively clearing the existing configuration.\n",
    "        \"\"\"\n",
    "        # Set the vector store to None.\n",
    "        self.vector_store = None\n",
    "\n",
    "        # Set the retriever to None.\n",
    "        self.retriever = None\n",
    "\n",
    "        # Set the processing chain to None.\n",
    "        self.chain = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
